---
layout: post
title: Linear Algebra
subtitle: my old friend
date: {}
author: Guy Davis
header-img: img/apostles.jpg
published: true
---

As part of explorations into machine learning, I've been brushing up on computer science basics starting with linear algebra.  Nice to see good old [eigenvectors](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors) again after so many years.

><p style="margin-top:0px;">An eigenvector of a square matrix A is a non-zero vector v such that multiplication by A alters only the scale of v:</p>
>
><p style="font-size:larger;text-align:center">Av = λv</p>
>
> The scalar λ is known as the eigenvalue corresponding to this eigenvector.
> 

So, why is this of any use?  Well, eigenvectors are used in [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition), which can be applied in [principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis), which is mentioned in this interesting [podcast on machine learning fundamentals](http://www.thetalkingmachines.com/blog/2015/7/2/solving-intelligence-and-machine-learning-fundamentals).

Excerpted from Chapter 2 of [Deep Learning](http://www.deeplearningbook.org/) by Ian Goodfellow,  Yoshua Bengio and Aaron Courville.
